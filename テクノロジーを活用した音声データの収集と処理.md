# テクノロジーを活用した音声データの収集と処理
## データ収集
言語/ライブラリ: JavaScript/jsPsych <br>
環境: Cognition.

### 産出実験（オンラインでの録音データ-->文字列化された音声）
```js
var jsPsych = initJsPsych({
    use_webaudio: false,
    show_progress_bar: true,
    on_finish: function() {
        jsPsych.data.displayData();
        // jsPsych.data.get().localSave('csv','mydata.csv');
    }
});

/* 前提ファイル
   画像: picture_test_1.png, picture_test_2.png
   音声: sound1.wav, sound2.wav
   動画: movie1.mp4
*/
var img_list   = ['picture_test_1.png', 'picture_test_2.png'];
var audio_list = ['sound1.wav', 'sound2.wav'];
var video_list = ['movie1.mp4'];

var preload = {
    type: jsPsychPreload,
    images: img_list,
    audio: audio_list,
    video: video_list
};

// 1. テキスト入力：氏名・言語歴
const info_text = {
    type: jsPsychSurveyText,
    questions: [
        { prompt: 'お名前（フルネーム）', name: 'name', placeholder: '例: 山田 太郎' },
        { prompt: '言語歴（使用言語や学習歴など）', name: 'language_history', rows: 3 }
    ]
};

// 2. ラジオボタン：母語・性別
const info_radio = {
    type: jsPsychSurveyMultiChoice,
    questions: [
        {
            prompt: '母語を選択してください',
            name: 'native_language',
            options: ['日本語', '英語', 'その他'],
            required: true, horizontal: true
        },
        {
            prompt: '性別を教えてください',
            name: 'gender',
            options: ['男性', '女性', '回答しない'],
            required: false, horizontal: true
        }
    ]
};

// 3. ラジオボタン：方言地域
const info_region = {
    type: jsPsychSurveyMultiChoice,
    questions: [
        {
            prompt: 'ご出身の地域（方言）を選択してください',
            name: 'dialect_region',
            options: [
                '北海道・東北', '関東', '中部',
                '関西', '中国・四国', '九州・沖縄'
            ],
            required: true, horizontal: false
        }
    ]
};

// 4. 複数選択：日常的に使用する言語
const info_checkbox = {
    type: jsPsychSurveyMultiSelect,
    questions: [
        {
            prompt: '日常的に使用する言語をすべて選んでください',
            name: 'languages_used',
            options: ['日本語', '英語', '中国語', '韓国語', 'その他'],
            required: true, horizontal: false
        }
    ]
};

// 5. リッカート尺度：録音・実験経験
const info_likert = {
    type: jsPsychSurveyLikert,
    questions: [
        {
            prompt: '言語学実験への参加経験はありますか？',
            name: 'experiment_experience',
            labels: ['全くない', '少しある', 'かなりある', '非常にある'],
            required: true, horizontal: true
        },
        {
            prompt: '英語での録音経験はどの程度ありますか？',
            name: 'recording_experience',
            labels: ['全くない', '少しある', 'かなりある', '非常にある'],
            required: true, horizontal: true
        }
    ]
};

// マイク初期化
var initialize_mic = {
    type: jsPsychInitializeMicrophone,
    device_select_message: 'マイクを選択',
    button_label: 'つぎへ'
};

// --- 刺激タスク ---
// ① 文のみ：最小対ペア読
var text_only = [
    {
        stimulus:
            '<h1>次の単語をそれぞれ2回ずつ音声で読み上げてください。</h1>' +
            '<p>おばさん<br>おばあさん<br>橋<br>箸</p>'
    }
];

// ② 文＋画像：画像説明
var text_and_image = img_list.map(function(filename) {
    return {
        stimulus:
            '<h1>画像を見て、何が描かれているか日本語で説明してください。</h1>' +
            '<img src="' + filename + '" width="300">'
    };
});

// ③ 音声のみ：聴取→リピート
var audio_only = audio_list.map(function(filename) {
    return {
        stimulus:
            '<h1>音声を聞いて、同じように読み上げてください。</h1>' +
            '<audio src="' + filename + '" controls autoplay></audio>'
    };
});

// ④ 動画のみ：口の動き模倣読
var video_only = video_list.map(function(filename) {
    return {
        stimulus:
            '<h1>動画を見て、話者の口の動きを真似しながら以下の文を読み上げてください。</h1>' +
            '<video src="' + filename + '" width="300" controls></video>'
    };
});

// 全刺激をまとめてランダマイズ
var item_list = [].concat(text_only, text_and_image, audio_only, video_only);
var randomized_item_list = jsPsych.randomization.shuffle(item_list);

// 録音設定
var record = {
    type: jsPsychHtmlAudioResponse,
    stimulus: jsPsych.timelineVariable('stimulus'),
    recording_duration: 10000,
    allow_playback: true,
    done_button_label: '録音終了',
    accept_button_label: 'つぎへ',
    record_again_button_label: '再録音'
};

var record_timeline = {
    timeline: [record],
    timeline_variables: randomized_item_list
};

// 終了
var goodbye = {
    type: jsPsychHtmlButtonResponse,
    stimulus: '<p>ご協力ありがとうございました。</p>',
    choices: ['データ送信'],
    response_ends_trial: true
};

// 最終タイムライン
var timeline = [
    preload,
    info_text,
    info_radio,
    info_region,
    info_checkbox,
    info_likert,
    initialize_mic,
    record_timeline,
    goodbye
];

jsPsych.run(timeline);
```
### 知覚実験 (おまけ)
## 前処理
言語: Python <br>
環境: Google Colaboratory (or Jupyter Notebookなど)
### Base64 to Wav
```py
!pip install pydub
import pandas as pd
import base64
from pydub import AudioSegment
import os

# 保存先のパスを指定する
file_path = '/content/'

# 保存先ディレクトリが存在しない場合は作成する
if not os.path.exists(file_path):
    os.makedirs(file_path)

# Base64でエンコードされたオーディオファイルに変換する関数を定義する
def convert_snd(s):
    # Base64をデコードする
    snd = base64.b64decode(s)
    return snd

# CSVファイルを読み込む
df = pd.read_csv('/content/test.csv')  # ローカル環境でのCSVファイルパスを指定

# 各行について処理を実行する。
for data in df.itertuples():
    # trial_type 列の値が 'html-audio-response' なら、
    if data.trial_type == 'html-audio-response':
        # ファイル名を生成し、
        file_base = str(data.run_id) + str(data.trial_index)
        # オーディオファイルを変換する
        snd = convert_snd(data.response)
        # webmファイルとして保存する
        temp_webm_path = os.path.join(file_path, file_base + '.webm')
        with open(temp_webm_path, 'wb') as f:
            f.write(snd)
        # webmをwavに変換する
        audio = AudioSegment.from_file(temp_webm_path, format="webm")
        wav_path = os.path.join(file_path, file_base + '.wav')
        audio.export(wav_path, format="wav")
```
### 16kHzへのリサンプリング
```py
import os
import librosa
import soundfile as sf

def resample_folder_with_librosa(input_folder, output_folder):
    for filename in os.listdir(input_folder):
        if filename.endswith(".wav"):
            input_path = os.path.join(input_folder, filename)
            y, sr = librosa.load(input_path, sr=None)
            y_resampled = librosa.resample(y, orig_sr=sr, target_sr=16000)
            base, ext = os.path.splitext(filename)
            output_filename = f"{base}_16000{ext}"
            output_path = os.path.join(output_folder, output_filename)
            os.makedirs(output_folder, exist_ok=True)
            sf.write(output_path, y_resampled, 16000)

# 音声ファイルのあるフォルダーのパスに変更する。
input_folder = "/content/drive/MyDrive"

# 音声ファイルを出力するフォルダーのパスに変更する。
output_folder = "/content/drive/MyDrive"
resample_folder_with_librosa(input_folder, output_folder)
```
## 書き起こし(と時刻情報を利用した切り出し, かな文字化)
言語/ライブラリ: Python/Whisper <br>
環境: Google Colaboratory (GPU利用)
### MFAでアノテーションしたい場合
### Juliusでアノテーションしたい場合
## アノテーション(Forced Alignment)
言語/ライブラリ: Python/Montreal Forced Aligner (MFA, 多言語対応), PySegmentKit (Perl/Julius のラッパー, 日本語向け) <br>
環境: Google Colaboratory or Jupyter Notebookなど (MFAは仮想環境でcondaによるインストールを推奨)
### Montreal Forced Aligner
### PySegmentKit (Julius のラッパー)
## 計測
言語/ライブラリ: Python/Parselmouth (Praatのラッパー), TextGridTools <br>
環境: Google Colaboratory (or Jupyter Notebookなど)
### Parselmouth
### TextGridTools
## データフレームと可視化
言語/ライブラリ: Python/Pandas, Matplotlib <br>
環境: Google Colaboratory (or Jupyter Notebookなど)
### データフレーム
### 可視化
